---
title: "GPU-accelerated HMM"
collection: projects 
type: "Projects"
permalink: /projects/2014-hmm-4
venue: "Northeastern University"
date: 2014-01-01
location: "Boston"
---

![protect overview](https://leimingyu.github.io/images/projects/gpu-hmm.png)

Hidden Markov Models (HMMs) is one of the most popular algorithms used in speech recognition. Training and testing an HMM is computationally intensive and time-consuming. We developed a GPU-accelerated HMM to improve execution efficiency.

An HMM requires 3 parameters: 
* the prior probability
* a state transition probability matrix
* an emission probability matrix
 
To train an HMM, 3 stages are implemented
* Forward stage, where we compute the likelihood of being in the state(i) at time t  after having observed the sequence from the start until t
* Backward stage, where we compute the likelihood of being in the state(i) at time t, given observing the rest of the sequence
* Expectation-Maximization (EM) stage, where we estimate the distribution over the states when the system is at time t, and maximize the probability of the observation sequence by updating the HMM parameters.



[Source Code](https://github.com/leimingyu/HMM_cuda)

One of the image is from [here](https://www.kisspng.com/png-speech-recognition-deep-learning-human-voice-convo-3713067/download-png.html).
